
# HR AI Lab â€” Experiments, Benchmarks & Responsible AI Framework

A collection of **practical, reproducible, HR-domain AI experiments** aligned with a vision for transforming People & Culture through AI.  
This repository demonstrates how an AI Engineer can build **scalable, ethical,
human-centric workforce tools** across recruitment, performance, career pathways,
and workflow automation.

**Note:**
- This project used AI for production of the source code and documentation

**ğŸš€ Now powered by Google Gemini 2.5 Flash-Lite (Free Tier)**
- All experiments use Google's latest Gemini models
- Built-in rate limiting for free tier (15 RPM)
- No API costs for experimentation
- See [Gemini Migration Guide](docs/gemini-migration-guide.md) for details

---

## ğŸ“ Repository Structure

```
hr-ai-lab/
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ recruitment_cv_screening/
â”‚   â”œâ”€â”€ interview_summarisation/
â”‚   â”œâ”€â”€ performance_review_drafter/
â”‚   â”œâ”€â”€ career_pathway_recommender/
â”‚   â”œâ”€â”€ workflow_agent_simulation/
â”‚   â”œâ”€â”€ ethical_ai_bias_tests/
â”‚   â”œâ”€â”€ hris_data_quality_agent/
â”‚   â”œâ”€â”€ culture_transformation_coach/
â”‚   â””â”€â”€ request_routing_agent/
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ synthetic_cvs/
â”‚   â”œâ”€â”€ interview_transcripts/
â”‚   â”œâ”€â”€ performance_notes/
â”‚   â”œâ”€â”€ job_families/
â”‚   â””â”€â”€ hris_samples/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ utils.py
â”‚
â””â”€â”€ results/
â”œâ”€â”€ leaderboard.csv
â””â”€â”€ dashboards/

```

---

# ğŸŒŸ Overview of Experiments
Each experiment includes:

- Purpose  
- Inputs & outputs  
- Sample prompts  
- Metrics & evaluation  
- Expected results  
- Ethical considerations  
- Diagram of flow  

---


# A. **CV Screening Benchmark (Recruitment Automation)**

### ğŸ¯ Goal  
Benchmark multiple LLMs for automated CV parsing & ranking while ensuring fairness.

### ğŸ“¥ Inputs  
- 20â€“50 anonymised or synthetic CVs  
- Target role descriptions  
- Skills ontology for an organisation (synthetic example provided)

### ğŸ“¤ Outputs  
- Structured JSON  
- Candidate-job similarity ranking  
- Evidence-backed reasoning

### ğŸ§ª Prompts  
```text
Extract the following fields from this CV:
- Skills (mapped to ontology)
- Years of experience
- Certifications
- Projects with outcomes
Return JSON only.
```

### ğŸ“Š Metrics

| Metric              | Definition                                        |
| ------------------- | ------------------------------------------------- |
| Extraction Accuracy | % of fields correctly extracted                   |
| Evidence Alignment  | Output must reference CV text, not hallucinations |
| Fairness Check      | Compare outputs with name/identity removed        |
| Latency             | ms per CV                                         |
| Cost                | tokens per CV                                     |

### ğŸ–¼ Diagram

```
PDF/DOCX CV â†’ LLM Parser â†’ Structured JSON â†’ Ranking Engine â†’ Report
```

### âœ… Expected Output

```json
{
  "name": "Candidate A",
  "skills": ["Bridge Design", "Revit"],
  "experience_years": 5,
  "fit_score": 0.83,
  "evidence": [
    "Worked on XYZ bridge reinforcement project in 2022"
  ]
}
```

---

# B. **Interview Summarisation Agent**

### ğŸ¯ Goal

Generate structured hiring summaries from interview transcripts.

### ğŸ“¥ Inputs

* Transcripts (synthetic 10â€“20)
* Organisation's competency rubric

### ğŸ“¤ Outputs

* Behavioural evidence
* STAR-format examples
* Hiring recommendation with justification

### ğŸ§ª Prompt Example

```text
Summarise this interview using the STAR method.
Identify evidence for each competency.
Do not invent information. Cite transcript lines.
```

### ğŸ“Š Metrics

* Consistency (same transcript â†’ same output)
* Evidence correctness
* Bias testing (swap candidate names)

### ğŸ“ˆ Expected Output

* 4â€“6 competency ratings
* 3 STAR examples
* Recommendation with risk flags

---

# C. **Performance Review Auto-Draft Assistant**

### ğŸ¯ Goal

Convert monthly manager notes into ready-to-use Workday performance drafts.

### ğŸ“¥ Inputs

* Manager notes (free text)
* Goal descriptions
* Competency library

### ğŸ“¤ Outputs

* Strengths summary
* Improvement opportunities
* Workday form fields

### ğŸ§ª Prompt

```text
Convert these check-in notes into a performance summary.
Use a neutral, professional tone.
Extract behaviour-based examples only.
```

### ğŸ“Š Metrics

* Manager satisfaction rating
* Coherence score
* Compliance (no value judgments, no bias)

---

# D. **Career Pathway Recommendation Engine**

### ğŸ¯ Goal

Recommend internal roles based on skills similarity + growth alignment.

### ğŸ“¥ Inputs

* Employee skill profile
* Internal job families
* Learning modules

### ğŸ“¤ Outputs

* Top 3 next roles
* Explanation for each recommendation
* 90-day development plan

### ğŸ§ª Algorithm

* Convert profiles + roles into embeddings
* Rank by cosine similarity
* Feed role shortlist into LLM for explanation/growth plan

### ğŸ§ª Prompt

```text
Explain why this employee is a good fit for these internal roles.
Generate personalised learning steps.
```

### ğŸ“Š Metrics

* Similarity accuracy
* Diversity of suggestions
* Explanation quality rating

---

# E. **HR Workflow Agent Simulation (Workday â†’ Teams â†’ Copilot)**

### ğŸ¯ Goal

Simulate cross-platform workflow automation.

### Example Scenario

**Event:** â€œNew hire onboarded in Workday.â€
**Outputs:**

* Teams message
* Calendar invites
* Onboarding checklist
* Copilot-ready summary

### ğŸ§ª Prompt Example

```text
Generate a welcome message for Teams.
Create a checklist for the hiring manager.
Propose calendar reminders.
Return JSON with UI-ready fields.
```

### ğŸ“Š Metrics

* Latency
* Task completeness
* Formatting correctness

---

# F. **Ethical AI Bias Stress Test Suite**

### ğŸ¯ Goal

Test HR prompts for fairness across sensitive attributes.

### ğŸ“¥ Inputs

* CVs, interview transcripts
* Name-flip variations
* Cultural background variations

### ğŸ“¤ Outputs

* Bias report
* Suggested mitigations
* Guardrail prompts

### ğŸ§ª Guardrail Prompt

```text
You must not use gender, ethnicity, or unrelated personal traits
to infer job suitability. Base all decisions strictly on evidence
in text. If uncertain, return "insufficient data".
```

### ğŸ“Š Metrics

* Output variance across identity-swaps
* Violation rate
* Guardrail effectiveness

### ğŸ“ˆ Expected Output

Bias deltas, e.g.:

```
Fit score (original): 0.78  
Fit score (name swapped): 0.77   â†’ âœ” Acceptable  
```

---

# G. **HRIS Data Quality Agent**

### ğŸ¯ Goal

Detect and fix HRIS inconsistencies.

### ğŸ“¥ Inputs

* Synthetic HRIS CSV
* Example anomalies

### ğŸ“¤ Outputs

* Suggested corrections
* Risk implications
* Confidence scores

### ğŸ§ª Example Prompt

```text
Identify errors in this HRIS dataset row.
Suggest corrections and explain potential downstream impacts.
```

### ğŸ“Š Metrics

* Error detection rate
* Correction quality
* Human QA agreement score

---

# H. **Culture Transformation Coach (Copilot Etiquette Assistant)**

### ğŸ¯ Goal

Improve communication safety & clarity across Teams / email.

### ğŸ“¥ Inputs

* Synthetic manager communications
* Psychological safety tone guidelines

### ğŸ“¤ Outputs

* Improved rewrite
* Justification
* Safety score

### ğŸ§ª Prompt

```text
Rewrite this message to be clear, constructive, and psychologically safe.
```

### ğŸ“Š Metrics

* Toxicity score reduction
* Clarity improvement
* Readability improvements (Flesch score)

---

# I. **HR Request Routing Agent (Autonomous HR Assistant)**

### ğŸ¯ Goal

Classify HR requests and take action.

### Example Inputs

* â€œI need parental leave infoâ€
* â€œI want to hire a structural engineerâ€
* â€œMy performance cycle is missingâ€

### Outputs

* Request category
* Required documents
* Policy link
* Next steps
* Escalation logic

### ğŸ§ª Prompt

```text
Classify this request into one of 12 HR categories.
Provide steps the user should take.
```

### ğŸ“Š Metrics

* Classification accuracy
* Completion rate
* Error rate

---

# ğŸ›¡ Responsible AI & Governance

### Principles embedded in all experiments:

* No unvalidated hiring recommendations
* No inferences based on sensitive attributes
* Evidence-only reasoning
* Human-in-the-loop for high-impact decisions
* Transparency: show *why* a model recommended something
* Drift monitoring: regular bias checks

---

# ğŸ§© Example Minimal Script (`scripts/run_experiment.py`)

```python
import json
from utils import call_llm, load_dataset

def run_cv_experiment():
    cvs = load_dataset("datasets/synthetic_cvs/")
    results = []

    for cv in cvs:
        output = call_llm(
            prompt=f"Extract fields:\n{cv['text']}",
            model="gpt-4.1-mini"
        )
        results.append(output)

    with open("results/cv_screening.json", "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    run_cv_experiment()
```

---

# ğŸ“Œ Summary

This repository demonstrates:

* Practical AI engineering
* Responsible AI
* HR domain understanding
* Systems thinking
* Integration thinking (Workday / M365 / Copilot)
* Culture transformation vision
* Benchmarking discipline

---

# ğŸš€ Quick Start

### Automated Setup (Recommended)

**macOS/Linux:**
```bash
chmod +x setup.sh
./setup.sh
```

**Windows:**
```cmd
setup.bat
```

### Manual Setup

1. **Create virtual environment:**
```bash
python3 -m venv .venv
source .venv/bin/activate  # macOS/Linux
# OR: .venv\Scripts\activate  # Windows
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Configure Google Gemini API key:**
```bash
cp .env.example .env
# Edit .env and add your GOOGLE_API_KEY from https://aistudio.google.com/app/apikey
```

4. **Run your first experiment:**
```bash
# CV Screening
python3 experiments/recruitment_cv_screening/cv_screener.py

# Or use the experiment runner
python scripts/run_experiment.py --experiment cv_screening --model gemini-2.5-flash-lite
```

5. **View results:**
```bash
python scripts/evaluate.py
```

### Project Structure

- `experiments/` - Individual experiment modules (9 total)
- `datasets/` - Synthetic datasets for testing
- `scripts/` - Core utilities and runners
- `results/` - Experiment outputs and leaderboards
- `docs/` - Documentation and implementation plans

### Rate Limiting

All experiments automatically respect Google Gemini's free tier rate limits:
- **15 RPM** (requests per minute) for Gemini 2.5 Flash-Lite
- **250,000 TPM** (tokens per minute)
- **1,000 RPD** (requests per day)

Rate limiting is handled transparently by the `LLMClient` class.

See [scripts/README.md](scripts/README.md) for detailed usage instructions.


